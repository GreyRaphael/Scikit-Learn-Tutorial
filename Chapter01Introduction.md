# Scikit-Learn Introduction

大纲:
- 回归模型: 数据→回归→智能, example: 房价预测
  - 线性回归
  - 多项式回归
- 分类模型: 数据→分类→智能, example: 情感分析
- 聚类模型: 数据→聚类→智能, example: wiki文本分析
- 推荐系统: 数据→协同过滤→智能, example: 音乐推荐系统
- 深度学习: 数据→深度学习→智能, example: 图像检索系统

Environment:
- pandas, scikit只能处理内存中的数据, 不能应用于大型的数据集
- sframe, graphlab create不仅能够处理内存中的数据集还可以处理大型数据集

## Regression model

example
- 房价:
  - 面积
  - 几室几厅
  - ...
- 学习成绩
  - 预习时间
  - 学习时间
  - 复习时间
  - 练习时间
- 股票价格
  - 近期股价
  - 新闻事件
  - 相关的商品
- 微博流行度
  - 粉丝数
  - 粉丝的粉丝数
  - 微博内容
  - 转发频率
- 预测房间空间某一点的温度
  - 现在的时间
  - 室外的温度
  - 其他传感器的温度
  - 通风口
  - ...

## classification

example
- 情感分析、评价打分
  - input: 文本内容
  - output: 正面、负面
- 新闻类别分类
  - input: 新闻内容
  - output: 新闻类别
- 垃圾邮件过滤
  - input: ip, sender, 邮件内容
  - output: 正常邮件、垃圾邮件
- 图像识别
  - input: pixel info
  - output: 名称
- 医疗诊断
  - input: 各种测试数据, DNA
  - output: 疾病种类
- 读懂正在想的内容
  - input: 核磁成像
  - outpu: 东西种类

阈值分类器: 统计正面和负面的单词个数
- if 正面个数>负面个数: 正面
- else: 负面

阈值分类器缺点:
- 无法区分单词不同程度: e.g. Great > Good
  - 通过学习分类器实现
- 单独的词语不够: e.g. not good
  - 通过更多特征解决

线性分类器:
- 对不同的单词加入权重，然后简单求和
  - if 求和>0: 正面
  - else: 负面

决策边界: 区分了正面和负面的预测
> ![](ch01/decision_boundary.png)
- 两个非零权重，线性分类器，决策边界时直线
- 三个非零权重，线性分类器，决策边界时平面
- 多个非零权重，线性分类器，决策边界时超平面
- 多个非零权重，非线性分类器，决策边界时超曲面

分类器工作过程: 学习得到单词权重，然后用权重去预测测试集
> ![](ch01/classfier01.png)  
> ![](ch01/classfier02.png)
- 测试集预测错误数/测试集总数= 误差
- 测试集预测正确数/测试集总数= 精度

精度的意义
- 对于二元分类器，随机猜测也有0.5的概率猜中，所以分类器预测精度应该>0.5，否则分类器毫无意义
- 对于k元分类器，模型预测精度应该>1/k
- 多数类预测要注意精度: e.g. 比如90%的邮件是垃圾邮件，那么将所有邮件预测为垃圾邮件，精度也能90%

二元混淆矩阵:
> ![](ch01/confusion_matrix.png)

二元混淆矩阵: 精度=对角线求和/总测试集
多元混淆矩阵: 精度=对角线求和/总测试集

学习曲线: 
- 一般数据量越多越好(数据质量是首要的)，但模型需要学习多少数据?
- 模型偏差的存在导致就算无穷的数据，模型偏差都不为0
- 复杂的模型需要更多的数据，模型偏差越小
> ![](ch01/learning_curve.png)

## Clustering

e.g. 根据现在阅读的文章，找到相似的感兴趣的文章
- 衡量文章相似度
- 对该文章进行如何搜索

词袋模型计算文档相似度：
- 词袋模型：忽略单词顺序，只是简单统计, 然后用一个统计向量(每个元素代表一个单词，元素的大小就是单词的计数)来描述
- 然后找一些其他文章，同样生成统计向量，将该统计向量与之前的进行**点乘**，数值越大，相似度越高

problem: 如果将两篇文章同时复制加到各自的末尾，文档长度变成2倍，点乘结果会更多，然而并不是更加相似
- 需要将统计向量归一化

problem: 如果文档出现生僻字，无法加入统计向量
- 关键词：局部常见(同一篇文档中反复出现)，全局罕见(所有文档中出现很少)
- 采用**TF-IDF**(逆向文档频率)方法解决: $log\frac{所有文档数目}{1+包含该词的文档数}$
  - 如果有一个词在所有文档都常见，那么该权重接近于0
  - 如果是一个生僻词，那么为了避免分母是0， 所以`+1`

K-mean:
- 初始化集群中心:Voronoi分割得到k个中心
- 然后调整中心位置(每个分区求平均)
- 每个区域不断计算与中心的距离，增加数据点或者减少数据点，重复上一个步骤，知道收敛

聚类其他应用:
- 组织网页搜索结果：搜索"狗",显示所有的狗
- 通过kmean估价相似条件的房子

